import os
import pandas as pd
from supabase import create_client, Client
from dotenv import load_dotenv
 
# Initialize Supabase client
def get_supabase_client():
    """Initialize and return Supabase client."""
    load_dotenv()
    url = os.getenv("SUPABASE_URL")
    key = os.getenv("SUPABASE_KEY")
   
    if not url or not key:
        raise ValueError("‚ùå Missing SUPABASE_URL or SUPABASE_KEY in .env")
       
    return create_client(url, key)

# Step 1: Create table if not exists

def create_table_if_not_exists():
    try:
        supabase = get_supabase_client()
       
        # Try to create the table using raw SQL
        create_table_sql = """
        DROP TABLE IF EXISTS public.churn_data;

CREATE TABLE public.churn_data (
    id BIGSERIAL PRIMARY KEY,
    tenure INTEGER,
    monthly_charges FLOAT,
    total_charges FLOAT,
    churn TEXT,
    internet_service TEXT,
    contract TEXT,
    payment_method TEXT,
    tenure_group TEXT,
    monthly_charge_segment TEXT,
    has_internet_service INTEGER,
    is_multi_line_user INTEGER,
    contract_type_code INTEGER
);


        """
       
        try:
            # Execute raw SQL to create table
            supabase.rpc('execute_sql', {'query': create_table_sql}).execute()
            print("‚úÖ Table 'churn_data' created or already exists")
        except Exception as e:
            print(f"‚ÑπÔ∏è  Note: {e}")
            print("‚ÑπÔ∏è  Table will be created on first insert")
 
    except Exception as e:
        print(f"‚ö†Ô∏è  Error checking/creating table: {e}")
        print("‚ÑπÔ∏è  Trying to continue with data insertion...")
 
# ------------------------------------------------------
# Step 2: Load CSV data into Supabase table
# ------------------------------------------------------
def load_to_supabase(staged_path: str, table_name: str = "churn_data"):
    """
    Load a transformed CSV into a Supabase table.
 
    Args:
        staged_path (str): Path to the transformed CSV file.
        table_name (str): Supabase table name. Default is 'titanic_data'.
    """
    # Convert to absolute path
    if not os.path.isabs(staged_path):
        staged_path = os.path.abspath(os.path.join(os.path.dirname(__file__), staged_path))
   
    print(f"üîç Looking for data file at: {staged_path}")
   
    if not os.path.exists(staged_path):
        print(f"‚ùå Error: File not found at {staged_path}")
        print("‚ÑπÔ∏è  Please run transform.py first to generate the transformed data")
        return
 
    try:
        # Initialize Supabase client
        supabase = get_supabase_client()
       
        # Read the CSV in chunks
        batch_size = 200  # Reduced batch size for better reliability
        df = pd.read_csv(staged_path)
        total_rows = len(df)
       
        print(f"üìä Loading {total_rows} rows into '{table_name}'...")
       
        # Process in batches
        for i in range(0, total_rows, batch_size):
            batch = df.iloc[i:i + batch_size].copy()
            # Convert NaN to None for proper NULL handling
            batch = batch.where(pd.notnull(batch), None)
            records = batch.to_dict('records')
           
            try:
                response = supabase.table(table_name).insert(records).execute()
                if hasattr(response, 'error') and response.error:
                    print(f"‚ö†Ô∏è  Error in batch {i//batch_size + 1}: {response.error}")
                else:
                    end = min(i + batch_size, total_rows)
                    print(f"‚úÖ Inserted rows {i+1}-{end} of {total_rows}")
            except Exception as e:
                print(f"‚ö†Ô∏è  Error in batch {i//batch_size + 1}: {str(e)}")
                continue
 
        print(f"üéØ Finished loading data into '{table_name}'.")
 
    except Exception as e:
        print(f"‚ùå Error loading data: {e}")
 
# ------------------------------------------------------
# Step 3: Run as standalone script
# ------------------------------------------------------
if __name__ == "__main__":
    # Path relative to the script location
    staged_csv_path = os.path.join("..", "data", "staged", "churn_transformed.csv")
    create_table_if_not_exists()  # Ensure table exists
    load_to_supabase(staged_csv_path)